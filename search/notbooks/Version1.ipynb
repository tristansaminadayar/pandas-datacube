{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper\n",
    "import numpy as np\n",
    "import time as tm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction qui retourne un DataFrame des résultats d'une requête SPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparql_dataframe(service: str, query: str, verbose: bool = False, text: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Helper function to convert SPARQL results into a Pandas data frame.\n",
    "\n",
    "\n",
    "    Credit: Douglas Fils\n",
    "\n",
    "    :param text: An optional text to add in the verbose mode\n",
    "    :param service: The link of the SPARQL service\n",
    "    :param query: The content of the query\n",
    "    :param verbose: If the function displays informations about execution\n",
    "    :return: A data frame with the answer of the server\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(tm.strftime(f\"[%H:%M:%S] Transmission {text} en cours...\"), end='')\n",
    "\n",
    "    sparql = SPARQLWrapper(service)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(\"json\")\n",
    "    processed_results: dict = sparql.query().convert()\n",
    "\n",
    "    if verbose:\n",
    "        print(tm.strftime(f\"\\r[%H:%M:%S] Transmission {text} réussi, conversion en Data Frame...\"), end='')\n",
    "\n",
    "    cols = processed_results['head']['vars']\n",
    "    out: list = []\n",
    "    for row in processed_results['results']['bindings']:\n",
    "        item: list = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        out.append(item)\n",
    "\n",
    "    if verbose:\n",
    "        print(tm.strftime(f\" Effectué\"))\n",
    "\n",
    "    return pd.DataFrame(out, columns=cols)\n",
    "\n",
    "\n",
    "def get_sparql_dataframe_complete(service: str, query: str, verbose: bool = False, step: int = 5000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    When in SELECTION mode, if the number of results is too high, the server is interrogated progressively\n",
    "\n",
    "    :param service: The link of the SPARQL service\n",
    "    :param query: The content of the query\n",
    "    :param verbose: If the function displays informations about execution\n",
    "    :param step: The max number of one query\n",
    "    :return: A data frame with the answer of the server\n",
    "    \"\"\"\n",
    "\n",
    "    sparql = SPARQLWrapper(service)\n",
    "    sparql.setReturnFormat(\"json\")\n",
    "\n",
    "    if query.strip().startswith(\"SELECT\"):  # Modifie the query to count the number of answer\n",
    "\n",
    "        if verbose:\n",
    "            print(tm.strftime(f\"[%H:%M:%S] Verification du nombre de résultats avant le requête\"))\n",
    "\n",
    "        start: int = 7\n",
    "        while query[start] != '?':\n",
    "            start += 1\n",
    "        end: int = start\n",
    "        while query[end] != ' ' and query[end] != '\\n':\n",
    "            end += 1\n",
    "        mot: str = query[start: end]\n",
    "        query_number: str = query.replace(mot, f\"(COUNT (*) as ?cnt)\", 1)\n",
    "        sparql.setQuery(query_number)\n",
    "        processed_results: dict = sparql.query().convert()\n",
    "        number_of_results: int = int(processed_results['results']['bindings'][0]['cnt']['value'])\n",
    "\n",
    "        if verbose:\n",
    "            print(tm.strftime(f\"[%H:%M:%S] Téléchargement de {number_of_results} résultats...\"))\n",
    "\n",
    "        if number_of_results > step:\n",
    "            query += f\" LIMIT {step}\"\n",
    "            return pd.concat([get_sparql_dataframe(service, query + f\" OFFSET {value}\", verbose,\n",
    "                                                   f\"{value:6} sur {number_of_results}\") for value in\n",
    "                              range(0, number_of_results, step)])\n",
    "\n",
    "    return get_sparql_dataframe(service, query, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On commence par chercher tout les différents types de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint: str = \"http://kaiko.getalp.org/sparql\"\n",
    "statement: str = \"\"\"SELECT DISTINCT ?t WHERE {?t a qb:DataSet }\"\"\"\n",
    "\n",
    "list_datasets = get_sparql_dataframe_complete(endpoint, statement).values.reshape(-1)  # We recovers all DataSets Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va grouper toutes les observations de chaque dataset dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs: dict = {}\n",
    "for dataset in list_datasets:\n",
    "    statement: str = f\"\"\"SELECT DISTINCT ?obs WHERE {'{'} ?obs qb:dataSet <{dataset}> {'}'}\"\"\"\n",
    "    result = get_sparql_dataframe_complete(endpoint, statement).values.reshape(-1)\n",
    "    name = dataset.split('/')[-1]\n",
    "    obs[name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour finir, on va créer un DataFrame pour chaque Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "STEP = 50\n",
    "DataFrames: dict = {}\n",
    "\n",
    "total: int = sum([len(item) for item in obs.values()])\n",
    "val: int = 0\n",
    "for key, values in zip(obs.keys(), obs.values()):\n",
    "    size: int = len(values)\n",
    "    tab: list = []\n",
    "    for i in range(0, size, STEP):\n",
    "        statement = f\"DESCRIBE \"\n",
    "        maxi: int = i + STEP\n",
    "        if maxi > size:\n",
    "            maxi = size\n",
    "        for uri in values[i:maxi]:\n",
    "            statement += \"<\" + uri + \"> \"\n",
    "        result = get_sparql_dataframe(endpoint, statement).sort_values(by='p').sort_values(by='s')[['p', 'o']].values\n",
    "        line_per_observation: int = result.shape[0] // (maxi - i)\n",
    "        for j in range(0, result.shape[0], line_per_observation):\n",
    "            temp = result[j:j + line_per_observation].T\n",
    "            a = pd.DataFrame(data=[list(temp[1])], columns=list(temp[0]))\n",
    "            tab.append(a)\n",
    "        val += (maxi - i)\n",
    "        print(tm.strftime(f\"\\r[%H:%M:%S] Téléchargement de {i} / {size} résultats... \"), end='')\n",
    "    print(\"\\nFusion de la base de donnée...\")\n",
    "    DataFrames[key] = pd.concat(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On sauvegarde les différentes base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in DataFrames.keys():\n",
    "    DataFrames[key].to_csv(f\"{key}_brut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrames[\"dbnaryStatisticsCube\"] = pd.read_csv(\"dbnaryStatisticsCube_brut.csv\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "def transformation_date(date: int) -> str:\n",
    "    date = str(date)\n",
    "    return date[:4] + '-' + date[4:6] + '-' + date[6:8]\n",
    "\n",
    "\n",
    "for key in DataFrames.keys():\n",
    "    DataFrames[key] = DataFrames[key].rename(columns=lambda item: item.split('#')[-1])\n",
    "    DataFrames[key][\"wiktionaryDumpVersion\"] = DataFrames[key][\"wiktionaryDumpVersion\"].map(transformation_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in DataFrames.keys():\n",
    "    DataFrames[key].to_csv(f\"{key}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrames[\"dbnaryStatisticsCube\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi= 200)\n",
    "plot_ = sns.lineplot(data = data[data[\"observationLanguage\"] == \"fr\"], x = \"wiktionaryDumpVersion\", y = \"translationsCount\")\n",
    "plot_ = sns.lineplot(data = data[data[\"observationLanguage\"] == \"fr\"], x = \"wiktionaryDumpVersion\", y = \"lexicalEntryCount\")\n",
    "plot_ = sns.lineplot(data = data[data[\"observationLanguage\"] == \"fr\"], x = \"wiktionaryDumpVersion\", y = \"lexicalSenseCount\")\n",
    "plot_ = sns.lineplot(data = data[data[\"observationLanguage\"] == \"fr\"], x = \"wiktionaryDumpVersion\", y = \"pageCount\")\n",
    "plt.xticks(rotation = 25)\n",
    "y_labels = plot_.get_yticks()\n",
    "plot_.set_yticklabels([int(y) for y in y_labels])\n",
    "for ind, label in enumerate(plot_.get_xticklabels()):\n",
    "  if ind % 20 == 0: # every 10th label is kept\n",
    "    label.set_visible(True)\n",
    "  else:\n",
    "    label.set_visible(False)\n",
    "plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbnary-datacube",
   "language": "python",
   "name": "ex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
